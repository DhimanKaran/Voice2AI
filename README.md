# Voice2AI

### Project Overview

This project was created for learning purposes to understand how small language models (SLMs) work.
It uses the Whisper framework to transcribe audio into text and then passes that transcribed text to the LLaMA framework for inference.

For demonstration, the project uses TinyLlama and Qwen models to run locally and display the output on-device.

During testing, the models produced minor inaccuracies or unrelated outputs (hallucinations) only in rare cases. This behavior is typical for language models without fine-tuning. Fine-tuning on task-specific data could help improve accuracy.

### Working

Audio ‚Üí Whisper ‚Üí Transcription ‚Üí LLaMA (TinyLlama/Qwen) ‚Üí Output


### üé• Demo
Question asked: ‚ÄúHow is AI helping humans? Do you think it is good?‚Äù

[Watch the demo - Qwen model](https://github.com/DhimanKaran/Voice2AI/raw/main/demo-videos/qwen-output.mp4)

[Watch the demo - TinyLlama model](https://github.com/DhimanKaran/Voice2AI/raw/main/demo-videos/tinyLlama-output.mp4)
